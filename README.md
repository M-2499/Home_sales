**Home-Sales-Big-Data-with-PySpark-SQL**

Through utilizing PySpark and Spark SQL on Google Colab, this project serves to determine key metrics about home sales data. Then, Spark is utilized to create temporary views, partition the data, cache and uncache a temporary table. Some of the data key metric questions that were answered:

What is the average price for a four-bedroom house sold for each year?

What is the average price of a home for each year it was built that has three bedrooms and three bathrooms?

What is the average price of a home for each year that has three bedrooms, three bathrooms, two floors, and is greater than or equal to 2,000 square feet?

What is the "view" rating for homes costing more than or equal to $350,000?

The project involved analyzing, cleansing, plotting, featurizing and modeling about a year's worth (20K) home sales in the Seattle metro-area from 2014-20125. I used the tools and libraries from Python, Numpy, Pandas, Seaborn, StatsModel, Sklearn and Scipy. We had a requirement to develop a multiple linear regression model as an initial approach to forecasting home prices. Here you can find my final business presentation and my Jupyter notebook.

Feel free to reach out if you have any questions or need further clarification. Happy coding!
